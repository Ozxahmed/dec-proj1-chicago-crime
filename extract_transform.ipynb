{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Chicago Crime - Extracting data\n",
    "\n",
    "link: https://dev.socrata.com/foundry/data.cityofchicago.org/x2n5-8w5q\n",
    "\n",
    "## To do\n",
    "\n",
    "- [x] sign up for app token [here](https://data.cityofchicago.org/login)\n",
    "- [x] create env file in main dir + Update jupyter notebook to use env file\n",
    "- [ ] Push to github\n",
    "- [ ] Recheck deduplicate for-loop (confirm we can dedup along 'case_' column)\n",
    "- [ ] Create logic for extraction, using the while loop and pagination in tandem with **order by `:id`**\n",
    "- [ ] Transformations\n",
    "- [ ] AWS\n",
    "- [ ] Database\n",
    "- [ ] Logic for UPSERT\n",
    "- [ ] Docker\n",
    "\n",
    "## Questions\n",
    "\n",
    "- [ ] How will we manage the env file in the docker container?\n",
    "- [ ] Who's App token are we going to use? Or are we to assume that the person using our container has to get their own app token?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv \n",
    "import json\n",
    "\n",
    "# configurations\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# .env\n",
    "dotenv_path = os.path.expanduser(\"~/Documents/DEC/dec-proj1-chicago-crime/.env\")  #Enter path to env file here\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# variables\n",
    "APP_TOKEN = os.environ.get(\"APP_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Logic (WIP)\n",
    "\n",
    "- retrieve MAX date_of_occurence from database\n",
    "- Round up to next day (2024-01-01T23:50:00.000 --> 2024-01-02T00:00:00.000), this becomes `start_time` variable\n",
    "- Add 23 hours, 59 mins, 59 seconds, 999 milliseconds to the time above using python datetime module. This become `end_time` variable\n",
    "- Query api using the 'BETWEEN .. AND ..' function using the `start_time` and `end_time` variables\n",
    "- Do transformations in python\n",
    "- upsert into database\n",
    "\n",
    "### Problems\n",
    "\n",
    "- we really won't be upserting, since we're only going to be uploading a day's worth of data at a time.\n",
    "\n",
    "### Thoughts from a balcony\n",
    "\n",
    "- If we upload only a day's worth of data at a time, every hour, that is going to be:\n",
    "  - 365 - 7 (1 year minus most recent 7 days) = **358** days in dataset\n",
    "  - 358 uploads / 24 uploads per day = Approx **15 days** to get all the data.\n",
    "- If we upload 7 days worth of data at a time, every hour...\n",
    "  - 358 / 7 = 52 uploads\n",
    "  - 52 / 24 = **2.5 days** to get all the data\n",
    "  - 7 days worth of data is ~5000 records per upload\n",
    "- So... we could upload 7 days worth of data at a time, and once we start getting no records, meaning we have reached the end of the dataset, we can fetch the min date from the dataset, and then start fetching 7 days worth of data again. This will be upserting then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Data (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will update this once done completing logic using `requests` library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop 'location' column \n",
    "\n",
    "drop 'location' column since it displays data we already have under the longitude and latitude columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('location', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplicate (WIP)\n",
    "\n",
    "I Need re-run the for loop below, there was an error in the logic! Load saved data from csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_all_cols = df[df.duplicated()]\n",
    "\n",
    "len(duplicates_all_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still duplicated records when filtering by case number, indicating that these cases have multiple records where one of the other columns have unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated(subset='case_')]\n",
    "\n",
    "len(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So Questions:**\n",
    "\n",
    "1. What columns have multiple unique values for cases with multiple records?\n",
    "2. And can we drop duplicates based on just the case number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view cases that have duplicate records\n",
    "duplicates = duplicates.sort_values('case_')\n",
    "\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, checking which columns have multiple unique values for each case number with duplicate records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of cols\n",
    "col_list = list()\n",
    "for col in duplicates.columns:\n",
    "    col_list.append(col)\n",
    "\n",
    "\n",
    "# initialize empty list to capture case numbers and what columns are different\n",
    "duplicates_breakdown = list()\n",
    "\n",
    "\n",
    "# Check what is duplicated\n",
    "for case in duplicates['case_']:  # cycle through cases that have duplicated records\n",
    "    duplicates_cols = list()  # initialize empty list to capture all columns that have different values within a set of duplicated records\n",
    "    for col in col_list:  # cycle through columns\n",
    "        if len((df[df['case_'] == case][col]).unique()) > 1:\n",
    "            duplicates_cols.append(col)\n",
    "    duplicates_breakdown.append(f'{case}, {duplicates_cols}')\n",
    "\n",
    "duplicates_breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(duplicates_breakdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answering questions from above:**\n",
    "\n",
    "1.\n",
    "2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Requests to extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- For pagination, **order by `:id`** (since there are multiple crime cases with the same date_of_occurrence), and use the `offset` and `limit` param in tandem. Increase offset by the value of the limit to grab the next page. Ex:\n",
    "\n",
    "  ```md\n",
    "  limit = 10\n",
    "  \n",
    "  pg 1\n",
    "  offset = 0\n",
    "\n",
    "  pg 2:\n",
    "  offset = 0 + 10 = 10\n",
    "\n",
    "  pg 3\n",
    "  offset = 10 + 10 = 20\n",
    "  ```\n",
    "\n",
    "- We can use a while loop, where we query the api as long as the len(data) we're receiving is equal to the limit value, and with each pass through increase offset by the limit value to get the next pg of data, until we reach the end of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Pagination below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 (pg1)\n",
    "\n",
    "import requests\n",
    "\n",
    "start_date = '2023-11-06T00:00:00.000'\n",
    "\n",
    "end_date = '2023-11-19T23:59:59.999'\n",
    "\n",
    "soql_date = f\"$where=date_of_occurrence between '{start_date}' and '{end_date}'\"\n",
    "\n",
    "limit = 1000\n",
    "\n",
    "offset = 0\n",
    "\n",
    "response = requests.get(f\"https://data.cityofchicago.org/resource/x2n5-8w5q.json?\"\n",
    "                        f\"$$app_token={APP_TOKEN}&\"\n",
    "                        f\"$order=:id\"  #a date of occurrence can have multiple cases, so better to use :id since that will lock the sequence of records.\n",
    "                        f\"&{soql_date}\"\n",
    "                        f\"&$limit={limit}\"\n",
    "                        f\"&$offset={offset}\")\n",
    "\n",
    "# print the message\n",
    "data1 = response.json()\n",
    "print(data1)\n",
    "assert response.status_code == 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.json_normalize(data1)\n",
    "\n",
    "print(len(df1))\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1['case_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 (pg2)\n",
    "\n",
    "offset = 0+1000  #pg2\n",
    "\n",
    "response = requests.get(f\"https://data.cityofchicago.org/resource/x2n5-8w5q.json?\"\n",
    "                        f\"$$app_token={APP_TOKEN}&\"\n",
    "                        f\"$order=:id\"\n",
    "                        f\"&{soql_date}\"\n",
    "                        f\"&$limit={limit}\"\n",
    "                        f\"&$offset={offset}\")\n",
    "\n",
    "# print the message\n",
    "data2 = response.json()\n",
    "print(data2)\n",
    "assert response.status_code == 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.json_normalize(data2)\n",
    "\n",
    "print(len(df2))\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2['case_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 (pg1 + pg2)\n",
    "\n",
    "full_limit = 1000 + 1000\n",
    "\n",
    "response = requests.get(f\"https://data.cityofchicago.org/resource/x2n5-8w5q.json?\"\n",
    "                        f\"$$app_token={APP_TOKEN}&\"\n",
    "                        f\"$order=:id\"\n",
    "                        f\"&{soql_date}\"\n",
    "                        f\"&$limit={full_limit}\"\n",
    "                        )\n",
    "\n",
    "# print the message\n",
    "data3 = response.json()\n",
    "print(data3)\n",
    "assert response.status_code == 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.json_normalize(data3)\n",
    "\n",
    "print(len(df3))\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3['case_'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing querying api for MINIMUM date_of_occurrence from dataset - **Success**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min date_of_occurence in dataset\n",
    "\n",
    "response = requests.get(f\"https://data.cityofchicago.org/resource/x2n5-8w5q.json?\"\n",
    "                        f\"$$app_token={APP_TOKEN}&\"\n",
    "                        f\"&$select=min(date_of_occurrence)\"\n",
    "                        )\n",
    "\n",
    "# print the message\n",
    "data3 = response.json()\n",
    "print(data3)\n",
    "assert response.status_code == 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
